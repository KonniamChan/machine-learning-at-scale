{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konniam Chan  \n",
    "Time of submission: 8:00pm PST   \n",
    "W261-3 Spring 2016  \n",
    "Week 1: Homework  \n",
    "January 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data refers to data sets so large that traditional computing frameworks are inadequate. One example of big data in the field of financial markets, where prices fluctuate on the micro-second level. The volume and velocity of thousands of securities make short-term trading suitable only to companies with big data infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Irreducible error**: This represents the noise (squared) in the data set. If we have multiple values (k of them) of y around the same x, we can estimate this quantity by taking the variance of y at $x_i$, then average for all $x_i$:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\frac{( y_{ik} - \\bar{y_i} )^2}{K-1} $$\n",
    "  \n",
    "**Bias**: Construct B bootstrap replicates of training set T, by drawing from T with replacement, and of length N (total number of data points). For each B, fit a model and make a predict on the out-of-bag points. Now, for each $x_i$, there is a set of $g_{i1}...g_{ik}$ predictions, with an average prediction $\\bar{g_i}$. The bias-squared can be calculated as:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} (\\bar{g_i} - y_i)^2 $$  \n",
    "  \n",
    "**Variance**: The variance can be calculated as:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\frac{( g_{ik} - \\bar{g_i} )^2}{K-1} $$  \n",
    "  \n",
    "**Model selection**:  \n",
    "For each polynomial model, calculate the expected out-of-sample error by summing the above 3 terms, i.e. $var + {bias}^2 + {noise}^2$. Pick the model with the lowest expected error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Convert data file from Windows to Unix line breaks\n",
    "# Run this on the command line: vi -c \"%s/^M/\\r/g | wq\" \"enronemail_1h.txt\"\n",
    "# with ^M = CTRL-V CTRL-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "# Command line inputs\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "findword_regex = re.compile(findword, re.IGNORECASE)\n",
    "# Add all occurrences of a single match\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():        \n",
    "        count += len(findword_regex.findall(line))\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.2\n",
    "import sys\n",
    "sum = 0\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r\n"
     ]
    }
   ],
   "source": [
    "# Test mapper and reducer with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "   that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "# Keep track of number of spam documents\n",
    "label_map = {\"1\":\"spam\", \"0\":\"ham\"}\n",
    "docs = {\"spam\":0, \"ham\":0}\n",
    "# Count of terms\n",
    "count_terms = {\"spam\":0, \"ham\":0}\n",
    "count_all = {\"spam\":0, \"ham\":0}\n",
    "# Assume one input word\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "findword_regex = re.compile(findword, re.IGNORECASE)\n",
    "# Iterate over documents\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split fields and count spam/ham docs\n",
    "        if len(line.strip().split(\"\\t\")) == 4:\n",
    "            doc_id, label, subject, body = line.strip().split(\"\\t\")\n",
    "        # Some lines have missing subjects\n",
    "        else:\n",
    "            subject = \"na\"\n",
    "            doc_id, label, body = line.strip().split(\"\\t\")            \n",
    "        label = label_map[label]\n",
    "        docs[label] += 1\n",
    "        # Strip quotes and concatenate subject and body\n",
    "        subject = subject.strip('\"')\n",
    "        body = body.strip('\"')\n",
    "        content = (subject + \" \" + body)\n",
    "        # Count total terms and matched terms\n",
    "        count_terms[label] += len(findword_regex.findall(content))\n",
    "        count_all[label] += len(content.split())        \n",
    "\n",
    "print '\\t'.join(map(str, [docs[\"spam\"], docs[\"ham\"], count_all[\"spam\"], count_all[\"ham\"]]))\n",
    "print '\\t'.join(map(str, [findword, count_terms[\"spam\"], count_terms[\"ham\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.3\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "docs = defaultdict(int)\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = defaultdict(int)\n",
    "\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        lines = myfile.readlines()\n",
    "        spam_counts, word_counts = lines[0], lines[1:]\n",
    "        # Sum total spam and ham docs, total term frequencies       \n",
    "        spam, ham, count_all_spam, count_all_ham = map(int, spam_counts.strip().split('\\t'))\n",
    "        docs[\"spam\"] += spam\n",
    "        docs[\"ham\"] += ham\n",
    "        count_all[\"spam\"] += count_all_spam\n",
    "        count_all[\"ham\"] += count_all_ham\n",
    "        # Sum individual term counts, assuming one word input\n",
    "        for line in word_counts:\n",
    "            findword, count_terms_spam, count_terms_ham = line.strip().split('\\t')\n",
    "            count_terms[\"spam\"][findword] += int(count_terms_spam)\n",
    "            count_terms[\"ham\"][findword] += int(count_terms_ham)\n",
    "\n",
    "# Establish priors\n",
    "priors = {}\n",
    "priors[\"spam\"] = count_all[\"spam\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "priors[\"ham\"] = count_all[\"ham\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "\n",
    "# NB classification\n",
    "NB_probs = priors.copy()\n",
    "for term in count_terms[\"spam\"]:\n",
    "    # Skip terms that don't exist in the training set\n",
    "    if count_terms[\"spam\"][term] == 0 and count_terms[\"ham\"][term] == 0:\n",
    "        continue\n",
    "    NB_probs[\"spam\"] *= count_terms[\"spam\"][term] / count_all[\"spam\"]\n",
    "    NB_probs[\"ham\"] *= count_terms[\"ham\"][term] / count_all[\"ham\"]\n",
    "predicted_class = \"spam\" if NB_probs[\"spam\"] > NB_probs[\"ham\"] else \"ham\"\n",
    "\n",
    "print (\"The predicted class is {}.\".format(predicted_class))\n",
    "print (\"Estimated NB probabilities for spam: {:.4f} and ham: {:.4f}.\"\n",
    "       .format(NB_probs[\"spam\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"]),\n",
    "               NB_probs[\"ham\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.8000 and ham: 0.2000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.  \n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "# Keep track of number of spam documents\n",
    "label_map = {\"1\":\"spam\", \"0\":\"ham\"}\n",
    "docs = {\"spam\":0, \"ham\":0}\n",
    "# Count of terms\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = {\"spam\":0, \"ham\":0}\n",
    "# Assume multiple input words\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].strip().split()\n",
    "findwords_regex = {term: re.compile(term, re.IGNORECASE) for term in findwords}\n",
    "# Iterate over documents\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split fields and count spam/ham docs\n",
    "        if len(line.strip().split(\"\\t\")) == 4:\n",
    "            doc_id, label, subject, body = line.strip().split(\"\\t\")\n",
    "        # Some lines have missing subjects\n",
    "        else:\n",
    "            subject = \"na\"\n",
    "            doc_id, label, body = line.strip().split(\"\\t\")            \n",
    "        label = label_map[label]\n",
    "        docs[label] += 1\n",
    "        # Strip quotes and concatenate subject and body\n",
    "        subject = subject.strip('\"')\n",
    "        body = body.strip('\"')\n",
    "        content = (subject + \" \" + body)\n",
    "        # Count total terms and matched terms\n",
    "        count_all[label] += len(content.split())\n",
    "        for term, regex in findwords_regex.items():\n",
    "            count_terms[label][term] += len(regex.findall(content))\n",
    "\n",
    "print '\\t'.join(map(str, [docs[\"spam\"], docs[\"ham\"], count_all[\"spam\"], count_all[\"ham\"]]))\n",
    "for term in findwords:\n",
    "    print '\\t'.join(map(str, [term, count_terms[\"spam\"][term], count_terms[\"ham\"][term]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.4\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "docs = defaultdict(int)\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = defaultdict(int)\n",
    "\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        lines = myfile.readlines()\n",
    "        spam_counts, word_counts = lines[0], lines[1:]\n",
    "        # Sum total spam and ham docs, total term frequencies       \n",
    "        spam, ham, count_all_spam, count_all_ham = map(int, spam_counts.strip().split('\\t'))\n",
    "        docs[\"spam\"] += spam\n",
    "        docs[\"ham\"] += ham\n",
    "        count_all[\"spam\"] += count_all_spam\n",
    "        count_all[\"ham\"] += count_all_ham\n",
    "        # Iterate through all terms\n",
    "        for line in word_counts:\n",
    "            term, count_terms_spam, count_terms_ham = line.strip().split('\\t')\n",
    "            count_terms[\"spam\"][term] += int(count_terms_spam)\n",
    "            count_terms[\"ham\"][term] += int(count_terms_ham)\n",
    "\n",
    "# Establish priors\n",
    "priors = {}\n",
    "priors[\"spam\"] = count_all[\"spam\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "priors[\"ham\"] = count_all[\"ham\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "\n",
    "# NB classification\n",
    "NB_probs = priors.copy()\n",
    "for term in count_terms[\"spam\"]:\n",
    "    # Skip terms that don't exist in the training set\n",
    "    if count_terms[\"spam\"][term] == 0 and count_terms[\"ham\"][term] == 0:\n",
    "        continue\n",
    "    NB_probs[\"spam\"] *= count_terms[\"spam\"][term] / count_all[\"spam\"]\n",
    "    NB_probs[\"ham\"] *= count_terms[\"ham\"][term] / count_all[\"ham\"]\n",
    "predicted_class = \"spam\" if NB_probs[\"spam\"] > NB_probs[\"ham\"] else \"ham\"\n",
    "\n",
    "print (\"The predicted class is {}.\".format(predicted_class))\n",
    "print (\"Estimated NB probabilities for spam: {:.4f} and ham: {:.4f}.\"\n",
    "       .format(NB_probs[\"spam\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"]),\n",
    "               NB_probs[\"ham\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.8000 and ham: 0.2000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 1.0000 and ham: 0.0000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"valium\"\n",
    "!./pNaiveBayes.sh 5 \"valium\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.5768 and ham: 0.4232.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"enlargementWithATypo\"\n",
    "!./pNaiveBayes.sh 5 \"enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words that aren't in the training data, the terms are simply skipped in the calculation of probabilities. The prediction would therefore be based on the priors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.9926 and ham: 0.0074.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and multiple words \"make a lot of money\"\n",
    "!./pNaiveBayes.sh 5 \"make a lot of money\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is ham.\r\n",
      "Estimated NB probabilities for spam: 0.4444 and ham: 0.5556.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and multiple words \"pay stub\",\n",
    "# that results in a ham prediction\n",
    "!./pNaiveBayes.sh 5 \"pay stub\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
