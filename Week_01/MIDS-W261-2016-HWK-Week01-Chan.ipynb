{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIDS W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konniam Chan  \n",
    "Time of submission: 11:30pm PST   \n",
    "W261-3 Spring 2016  \n",
    "Week 1: Homework  \n",
    "January 21, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Big data refers to data sets so large that traditional computing frameworks are inadequate. The time it would take if one were to process these data sets on a single laptop would be unreasonable. In particular, assume a laptop has 1TB of storage, it would take 3 hours to read this data set. Processing the data would be impossible with a few gig's of ram. \n",
    "\n",
    "One example of big data in the field of financial markets, where prices fluctuate on the micro-second level. The volume and velocity of thousands of securities make short-term trading suitable only to companies with big data infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-sample error of our machine learning models can be decomposed into 3 parts: irreducible error, bias, and variance.\n",
    "\n",
    "**Irreducible error**: This represents the noise (squared) in the data set. If we have multiple values (k of them) of y around the same x, we can estimate this quantity by taking the variance of y at $x_i$, then average for all $x_i$:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\frac{( y_{ik} - \\bar{y_i} )^2}{K-1} $$\n",
    "  \n",
    "**Bias**: Bias represents the inability of the model to estimate the target function. Here is how to estiamte it from a data set T:  \n",
    "Construct B bootstrap replicates of training set T, by drawing from T with replacement, and of length N (total number of data points). For each B, fit a model and make a predict on the out-of-bag points. Now, for each $x_i$, there is a set of $g_{i1}...g_{ik}$ predictions, with an average prediction $\\bar{g_i}$. The bias-squared can be calculated as:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} (\\bar{g_i} - y_i)^2 $$  \n",
    "  \n",
    "**Variance**: Variance represents the variability of the different hypotheses that can be drawn given different data sets. The variance can be calculated as:  \n",
    "$$\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\frac{( g_{ik} - \\bar{g_i} )^2}{K-1} $$  \n",
    "\n",
    "The follow graphics ([Caltech's Learning from Data Lecture 8](http://work.caltech.edu/slides/slides08.pdf)) displays the bias and variance characteristics of a constant model (H0) and a linear model (H1). The target function is a simulated sine function. Because the H0 model is so simple (1 parameter), the model cannot approximate the sine curve well (the bias is high). However, the variance is low (the spread is the gray area). The H1 model is a little more complicated (2 parameters). While the average hypothesis seems to follow the sine curve better (low bias), the variability of different hypotheses is much higher. The end result is that the simpler model H0 works better in this case, because the sum of bias and variance is lower.  \n",
    "  \n",
    "<img src=\"LFD_1.png\">\n",
    "<img src=\"LFD_2.png\">\n",
    "<img src=\"LFD_3.png\">\n",
    "  \n",
    "The above plots show the trade-off between bias and variance. Given a fixed number of training examples, the more complex a model is, the lower the bias, but it is the harder for a particular hypothesis to be close to the average hypothesis (higher variance). A model needs to be matched to the size of the training data. It needs to be as complex as needed, but not more complex, as overfitting will negatively affect the expected error (from substantial increases of variance).  \n",
    "\n",
    "**Model selection**:  \n",
    "For each polynomial model, calculate the expected out-of-sample error by plotting the bias and variance terms (and their sum), assuming noise is zero. If we plot the error as a function of the number of degrees (model complexity), we see the trade-off between bias and variance. We want to minimize the expected error, so we would pick the model with degree 3.\n",
    "<img src=\"BV_curve.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.\n",
    "\n",
    "   To do so, make sure that\n",
    "   \n",
    "   - mapper.py counts all occurrences of a single word, and\n",
    "   - reducer.py collates the counts of the single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (Optional) Convert data file from Windows to Unix line breaks\n",
    "# Run this on the command line: vi -c \"%s/^M/\\r/g | wq\" \"enronemail_1h.txt\"\n",
    "# with ^M = CTRL-V CTRL-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "# Command line inputs\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "findword_regex = re.compile(findword, re.IGNORECASE)\n",
    "# Add all occurrences of a single match\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():        \n",
    "        count += len(findword_regex.findall(line))\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.2\n",
    "import sys\n",
    "sum = 0\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            sum += int(line)\n",
    "print sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\r\n"
     ]
    }
   ],
   "source": [
    "# Test mapper and reducer with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.\n",
    "To do so, make sure that\n",
    "   \n",
    "   - mapper.py and\n",
    "   - reducer.py \n",
    "\n",
    "   that performs a single word Naive Bayes classification. For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   NOTE if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "# Keep track of number of spam documents\n",
    "label_map = {\"1\":\"spam\", \"0\":\"ham\"}\n",
    "docs = {\"spam\":0, \"ham\":0}\n",
    "# Count of terms\n",
    "count_terms = {\"spam\":0, \"ham\":0}\n",
    "count_all = {\"spam\":0, \"ham\":0}\n",
    "# Assume one input word\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "findword_regex = re.compile(findword, re.IGNORECASE)\n",
    "# Iterate over documents\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split fields and count spam/ham docs\n",
    "        if len(line.strip().split(\"\\t\")) == 4:\n",
    "            doc_id, label, subject, body = line.strip().split(\"\\t\")\n",
    "        # Some lines have missing subjects\n",
    "        else:\n",
    "            subject = \"na\"\n",
    "            doc_id, label, body = line.strip().split(\"\\t\")            \n",
    "        label = label_map[label]\n",
    "        docs[label] += 1\n",
    "        # Strip quotes and concatenate subject and body\n",
    "        subject = subject.strip('\"')\n",
    "        body = body.strip('\"')\n",
    "        content = (subject + \" \" + body)\n",
    "        # Count total terms and matched terms\n",
    "        count_terms[label] += len(findword_regex.findall(content))\n",
    "        count_all[label] += len(content.split())        \n",
    "\n",
    "print '\\t'.join(map(str, [docs[\"spam\"], docs[\"ham\"], count_all[\"spam\"], count_all[\"ham\"]]))\n",
    "print '\\t'.join(map(str, [findword, count_terms[\"spam\"], count_terms[\"ham\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.3\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "docs = defaultdict(int)\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = defaultdict(int)\n",
    "\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        lines = myfile.readlines()\n",
    "        spam_counts, word_counts = lines[0], lines[1:]\n",
    "        # Sum total spam and ham docs, total term frequencies       \n",
    "        spam, ham, count_all_spam, count_all_ham = map(int, spam_counts.strip().split('\\t'))\n",
    "        docs[\"spam\"] += spam\n",
    "        docs[\"ham\"] += ham\n",
    "        count_all[\"spam\"] += count_all_spam\n",
    "        count_all[\"ham\"] += count_all_ham\n",
    "        # Sum individual term counts, assuming one word input\n",
    "        for line in word_counts:\n",
    "            findword, count_terms_spam, count_terms_ham = line.strip().split('\\t')\n",
    "            count_terms[\"spam\"][findword] += int(count_terms_spam)\n",
    "            count_terms[\"ham\"][findword] += int(count_terms_ham)\n",
    "\n",
    "# Establish priors\n",
    "priors = {}\n",
    "priors[\"spam\"] = count_all[\"spam\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "priors[\"ham\"] = count_all[\"ham\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "\n",
    "# NB classification\n",
    "NB_probs = priors.copy()\n",
    "for term in count_terms[\"spam\"]:\n",
    "    # Skip terms that don't exist in the training set\n",
    "    if count_terms[\"spam\"][term] == 0 and count_terms[\"ham\"][term] == 0:\n",
    "        continue\n",
    "    NB_probs[\"spam\"] *= count_terms[\"spam\"][term] / count_all[\"spam\"]\n",
    "    NB_probs[\"ham\"] *= count_terms[\"ham\"][term] / count_all[\"ham\"]\n",
    "predicted_class = \"spam\" if NB_probs[\"spam\"] > NB_probs[\"ham\"] else \"ham\"\n",
    "\n",
    "print (\"The predicted class is {}.\".format(predicted_class))\n",
    "print (\"Estimated NB probabilities for spam: {:.4f} and ham: {:.4f}.\"\n",
    "       .format(NB_probs[\"spam\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"]),\n",
    "               NB_probs[\"ham\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\t56\t18919\t13881\r\n",
      "assistance\t8\t2\r\n"
     ]
    }
   ],
   "source": [
    "# Show sample mapper output\n",
    "!./mapper.py enronemail_1h.txt \"assistance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.8000 and ham: 0.2000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results.  \n",
    "   To do so, make sure that\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "   performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/env python\n",
    "## mapper.py\n",
    "## Author: Konniam Chan\n",
    "## Description: mapper code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "from collections import defaultdict\n",
    "# Keep track of number of spam documents\n",
    "label_map = {\"1\":\"spam\", \"0\":\"ham\"}\n",
    "docs = {\"spam\":0, \"ham\":0}\n",
    "# Count of terms\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = {\"spam\":0, \"ham\":0}\n",
    "# Assume multiple input words\n",
    "filename = sys.argv[1]\n",
    "findwords = sys.argv[2].strip().split()\n",
    "findwords_regex = {term: re.compile(term, re.IGNORECASE) for term in findwords}\n",
    "# Iterate over documents\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split fields and count spam/ham docs\n",
    "        if len(line.strip().split(\"\\t\")) == 4:\n",
    "            doc_id, label, subject, body = line.strip().split(\"\\t\")\n",
    "        # Some lines have missing subjects\n",
    "        else:\n",
    "            subject = \"na\"\n",
    "            doc_id, label, body = line.strip().split(\"\\t\")            \n",
    "        label = label_map[label]\n",
    "        docs[label] += 1\n",
    "        # Strip quotes and concatenate subject and body\n",
    "        subject = subject.strip('\"')\n",
    "        body = body.strip('\"')\n",
    "        content = (subject + \" \" + body)\n",
    "        # Count total terms and matched terms\n",
    "        count_all[label] += len(content.split())\n",
    "        for term, regex in findwords_regex.items():\n",
    "            count_terms[label][term] += len(regex.findall(content))\n",
    "\n",
    "print '\\t'.join(map(str, [docs[\"spam\"], docs[\"ham\"], count_all[\"spam\"], count_all[\"ham\"]]))\n",
    "for term in findwords:\n",
    "    print '\\t'.join(map(str, [term, count_terms[\"spam\"][term], count_terms[\"ham\"][term]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/env python\n",
    "## reducer.py\n",
    "## Author: Konniam Chan\n",
    "## Description: reducer code for HW1.4\n",
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "docs = defaultdict(int)\n",
    "count_terms = {\"spam\": defaultdict(int), \"ham\": defaultdict(int)}\n",
    "count_all = defaultdict(int)\n",
    "\n",
    "# Iterate through files with intermediate counts\n",
    "for filename in sys.argv[1:]:\n",
    "    with open(filename, \"r\") as myfile:\n",
    "        lines = myfile.readlines()\n",
    "        spam_counts, word_counts = lines[0], lines[1:]\n",
    "        # Sum total spam and ham docs, total term frequencies       \n",
    "        spam, ham, count_all_spam, count_all_ham = map(int, spam_counts.strip().split('\\t'))\n",
    "        docs[\"spam\"] += spam\n",
    "        docs[\"ham\"] += ham\n",
    "        count_all[\"spam\"] += count_all_spam\n",
    "        count_all[\"ham\"] += count_all_ham\n",
    "        # Iterate through all terms\n",
    "        for line in word_counts:\n",
    "            term, count_terms_spam, count_terms_ham = line.strip().split('\\t')\n",
    "            count_terms[\"spam\"][term] += int(count_terms_spam)\n",
    "            count_terms[\"ham\"][term] += int(count_terms_ham)\n",
    "\n",
    "# Establish priors\n",
    "priors = {}\n",
    "priors[\"spam\"] = count_all[\"spam\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "priors[\"ham\"] = count_all[\"ham\"] / (count_all[\"spam\"] + count_all[\"ham\"])\n",
    "\n",
    "# NB classification\n",
    "NB_probs = priors.copy()\n",
    "for term in count_terms[\"spam\"]:\n",
    "    # Skip terms that don't exist in the training set\n",
    "    if count_terms[\"spam\"][term] == 0 and count_terms[\"ham\"][term] == 0:\n",
    "        continue\n",
    "    NB_probs[\"spam\"] *= count_terms[\"spam\"][term] / count_all[\"spam\"]\n",
    "    NB_probs[\"ham\"] *= count_terms[\"ham\"][term] / count_all[\"ham\"]\n",
    "predicted_class = \"spam\" if NB_probs[\"spam\"] > NB_probs[\"ham\"] else \"ham\"\n",
    "\n",
    "print (\"The predicted class is {}.\".format(predicted_class))\n",
    "print (\"Estimated NB probabilities for spam: {:.4f} and ham: {:.4f}.\"\n",
    "       .format(NB_probs[\"spam\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"]),\n",
    "               NB_probs[\"ham\"]/(NB_probs[\"spam\"]+NB_probs[\"ham\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\t56\t18919\t13881\r\n",
      "the\t993\t695\r\n",
      "business\t47\t23\r\n",
      "proposal\t7\t2\r\n"
     ]
    }
   ],
   "source": [
    "# Show example mapper output\n",
    "!./mapper.py enronemail_1h.txt \"the business proposal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.8000 and ham: 0.2000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"assistance\"\n",
    "!./pNaiveBayes.sh 5 \"assistance\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 1.0000 and ham: 0.0000.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"valium\"\n",
    "!./pNaiveBayes.sh 5 \"valium\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.5768 and ham: 0.4232.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and a single word \"enlargementWithATypo\"\n",
    "!./pNaiveBayes.sh 5 \"enlargementWithATypo\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For words that aren't in the training data, the terms are simply skipped in the calculation of probabilities. The prediction would therefore be based on the priors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is spam.\r\n",
      "Estimated NB probabilities for spam: 0.9926 and ham: 0.0074.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and multiple words \"make a lot of money\"\n",
    "!./pNaiveBayes.sh 5 \"make a lot of money\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is ham.\r\n",
      "Estimated NB probabilities for spam: 0.4444 and ham: 0.5556.\r\n"
     ]
    }
   ],
   "source": [
    "# Test NB classifier with 5 processes and multiple words \"pay stub\",\n",
    "# that results in a ham prediction\n",
    "!./pNaiveBayes.sh 5 \"pay stub\"\n",
    "!cat enronemail_1h.txt.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
